{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ff5c36",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f2de4",
   "metadata": {},
   "source": [
    "# =>\n",
    "Polynomial functions and kernel functions are related in the context of machine learning algorithms, particularly in support vector machines (SVMs) and kernelized models. Let's break down the relationship:\n",
    "\n",
    "### 1. **Basis for Polynomial Kernels:**\n",
    "   - Polynomial functions are a type of basis function that can be used in the context of kernelized machine learning algorithms.\n",
    "   - In SVMs, the idea is to map the input data into a higher-dimensional space where a linear decision boundary may be more effective.\n",
    "\n",
    "### 2. **Kernel Trick:**\n",
    "   - The kernel trick is a technique used in machine learning to implicitly map data into higher-dimensional spaces without explicitly computing the transformed feature vectors.\n",
    "   - Polynomial functions are commonly used as kernel functions in this context.\n",
    "\n",
    "### 3. **Polynomial Kernel:**\n",
    "   - The polynomial kernel is a specific type of kernel function that uses a polynomial as the basis function.\n",
    "   - The polynomial kernel function is defined as \\(K(x, y) = (x \\cdot y + c)^d\\), where \\(d\\) is the degree of the polynomial and \\(c\\) is a constant.\n",
    "\n",
    "### 4. **Non-Linearity:**\n",
    "   - Polynomial kernels introduce non-linearity to the decision boundary of linear models. This can be useful when dealing with data that is not linearly separable in the original feature space.\n",
    "\n",
    "### 5. **Representation in Feature Space:**\n",
    "   - The polynomial kernel implicitly represents the data in a higher-dimensional feature space, allowing the algorithm to capture more complex relationships.\n",
    "\n",
    "### 6. **Trade-off:**\n",
    "   - While higher-degree polynomial kernels can capture more complex patterns, they also come with the risk of overfitting the data. The choice of the degree \\(d\\) is a hyperparameter that needs to be carefully tuned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beccb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3d8e916",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4748bb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0, gamma='scale')\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f5845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15f9c6",
   "metadata": {},
   "source": [
    "=>\n",
    "In Support Vector Regression (SVR), epsilon (ε) is a parameter associated with the width of the tube around the predicted values within which no penalty is associated with the training points. It is part of the formulation of the epsilon-insensitive loss function, which determines the insensitive zone within which errors are not penalized. This loss function is used to train the SVR model.\n",
    "\n",
    "The epsilon-insensitive loss function is defined as follows:\n",
    "\n",
    "- If the absolute difference between the predicted output and the actual target is less than or equal to ε, the loss is 0.\n",
    "- If the absolute difference is greater than ε, the loss is proportional to the difference minus ε.\n",
    "\n",
    "Now, let's discuss how increasing the value of epsilon affects the number of support vectors:\n",
    "\n",
    "1. **Smaller Epsilon (Tight Tube):**\n",
    "   - A smaller epsilon implies a tighter tube around the predicted values.\n",
    "   - This can lead to a larger number of support vectors because the model is more sensitive to errors and tries to fit the training data more closely.\n",
    "   - The model might capture noise in the data, and the decision boundary may follow the training points more closely.\n",
    "\n",
    "2. **Larger Epsilon (Wider Tube):**\n",
    "   - A larger epsilon implies a wider tube around the predicted values.\n",
    "   - This can result in a smaller number of support vectors because the model allows for more errors within the insensitive zone.\n",
    "   - The model is less sensitive to individual data points, and it focuses on capturing the overall trend in the data rather than fitting each point precisely.\n",
    "\n",
    "3. **Impact on Generalization:**\n",
    "   - A smaller epsilon may lead to overfitting, where the model fits the training data too closely but may not generalize well to new, unseen data.\n",
    "   - A larger epsilon promotes a more robust model that generalizes better to new data by allowing some flexibility in the fitting process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba66cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321525b",
   "metadata": {},
   "source": [
    "# =>\n",
    "Support Vector Regression (SVR) is a powerful regression technique, and the choice of its parameters can significantly impact its performance. Here's an explanation of the key SVR parameters and how they influence the model:\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   - **Role:** The kernel function determines the type of function used to map the input data into a higher-dimensional space.\n",
    "   - **Choices:**\n",
    "      - **Linear (default):** \\( K(x, y) = x^T \\cdot y \\)\n",
    "      - **Polynomial:** \\( K(x, y) = (x \\cdot y + c)^d \\)\n",
    "      - **Radial Basis Function (RBF or Gaussian):** \\( K(x, y) = \\exp(-\\gamma \\|x-y\\|^2) \\), where \\(\\gamma\\) is a positive parameter.\n",
    "   - **Considerations:**\n",
    "      - Linear kernels are suitable for linear relationships.\n",
    "      - Polynomial kernels capture non-linear relationships, with the degree (\\(d\\)) controlling the complexity.\n",
    "      - RBF kernels are versatile and effective for various patterns, but tuning \\(\\gamma\\) is crucial.\n",
    "\n",
    "2. **C Parameter:**\n",
    "   - **Role:** The C parameter controls the trade-off between achieving a smooth fit and fitting the training data points.\n",
    "   - **Effect:**\n",
    "      - Smaller \\(C\\) values lead to a smoother decision surface, allowing more training errors.\n",
    "      - Larger \\(C\\) values aim for a tighter fit, penalizing deviations from the actual values more strongly.\n",
    "   - **Examples:**\n",
    "      - Increase \\(C\\) if you suspect overfitting or want a closer fit to the training data.\n",
    "      - Decrease \\(C\\) to allow for a more flexible and smoother model.\n",
    "\n",
    "3. **Epsilon Parameter (ε):**\n",
    "   - **Role:** The epsilon parameter (\\( \\varepsilon \\)) defines the width of the epsilon-insensitive tube where no penalty is associated with errors.\n",
    "   - **Effect:**\n",
    "      - Smaller \\( \\varepsilon \\) values result in a narrower tube, making the model less tolerant to errors.\n",
    "      - Larger \\( \\varepsilon \\) values create a wider tube, allowing for more errors within the insensitive zone.\n",
    "   - **Examples:**\n",
    "      - Increase \\( \\varepsilon \\) if you want to allow more flexibility in fitting the data and avoid overfitting.\n",
    "      - Decrease \\( \\varepsilon \\) for a more precise fit when you have confidence in the noise level of your data.\n",
    "\n",
    "4. **Gamma Parameter:**\n",
    "   - **Role:** The gamma parameter (\\( \\gamma \\)) defines how far the influence of a single training example reaches in the RBF kernel.\n",
    "   - **Effect:**\n",
    "      - Smaller \\( \\gamma \\) values result in a broader influence, making the model more general.\n",
    "      - Larger \\( \\gamma \\) values lead to a more localized influence, making the model sensitive to individual data points.\n",
    "   - **Examples:**\n",
    "      - Increase \\( \\gamma \\) for more complex patterns and when there are fewer support vectors.\n",
    "      - Decrease \\( \\gamma \\) for smoother decision surfaces and when there are more support vectors.\n",
    "\n",
    "**Examples of Parameter Tuning:**\n",
    "- **If the model is too complex (overfitting):**\n",
    "  - Decrease \\(C\\) to allow for a smoother decision surface.\n",
    "  - Increase \\( \\varepsilon \\) to make the model less sensitive to errors.\n",
    "  - Increase \\( \\gamma \\) for a broader influence in the RBF kernel.\n",
    "\n",
    "- **If the model is too simple (underfitting):**\n",
    "  - Increase \\(C\\) to tighten the fit to the training data.\n",
    "  - Decrease \\( \\varepsilon \\) for a more precise fit.\n",
    "  - Decrease \\( \\gamma \\) for a broader influence in the RBF kernel.\n",
    "\n",
    "Parameter tuning often involves using techniques like cross-validation and grid search to find the optimal values for a given dataset. It's important to carefully consider the characteristics of the data and the desired model behavior when adjusting these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613a4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b502bee",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    " Import the necessary libraries and load the dataseg\n",
    " Split the dataset into training and testing setZ\n",
    "Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    " Create an instance of the SVC classifier and train it on the training datW\n",
    " hse the trained classifier to predict the labels of the testing datW\n",
    " Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    " Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    " Train the tuned classifier on the entire dataseg\n",
    " Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd91a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Best Hyperparameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tuned_svm_classifier.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib  # To save the trained classifier to a file\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling in this case)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels on the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Evaluate the performance using other metrics (precision, recall, F1-score)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_classifier = grid_search.best_estimator_\n",
    "tuned_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(tuned_classifier, 'tuned_svm_classifier.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e2343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b7eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305fd43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963727ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
